# NN06: Backpropagation

> [!TIP]
>
> <details open>
>
> <summary><strong>ðŸŽ¦ Videos</strong></summary>
>
> - [NN6.1 - MLP Backpropagation 1](https://youtu.be/G9x75THjueQ)
> - [NN6.2 - MLP Backpropagation 2](https://youtu.be/9Ku0dJ8pGrU)
> - [NN6.3 - MLP Zusammenfassung](https://youtu.be/uvT4WPIIkwQ)
>
> </details>

> [!NOTE]
>
> <details open>
>
> <summary><strong>ðŸ–‡ Weitere Unterlagen</strong></summary>
>
> - [NN06-MLP_Backpropagation.pdf](https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/lecture/nn/files/NN06-MLP_Backpropagation.pdf)
> - [NN06.2-MLP_Backpropagation_Beispiel.pdf](https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/lecture/nn/files/NN06.2-MLP_Backpropagation_Beispiel.pdf)
>
> </details>

## Kurze Ãœbersicht

### ForwÃ¤rts- und RÃ¼ckwÃ¤rtslauf

- Im ForwÃ¤rtslauf (engl. forward pass oder forward propagation) wird ein
  einzelner **ForwÃ¤rtsschritt** von Schicht $`[l-1]`$ auf Schicht
  $`[l]`$ wie folgt berechnet: Dabei bezeichnet $`g`$ die
  Aktivierungsfunktion (z.B. Sigmoid oder ReLU).

``` math
Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} \tag{1}
```

``` math
A^{[l]} = g(Z^{[l]}) \tag{2}
```

- Im RÃ¼ckwÃ¤rtslauf (engl. *backpropagation*) werden in einem einzelnen
  **RÃ¼ckwÃ¤rtsschritt** von Schicht $`[l]`$ auf Schicht $`[l-1]`$ die
  folgenden Gradienten berechnet:

  Dabei steht â€œ$`*`$â€ fÃ¼r die elementweise Multiplikation.

``` math
dZ^{[l]} := \frac{\partial J }{\partial Z^{[l]}} = dA^{[l]} * g'(Z^{[l]}) \tag{3}
```

``` math
dW^{[l]} := \frac{\partial J }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T} \tag{4}
```

``` math
db^{[l]} := \frac{\partial J }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{[l](i)}\tag{5}
```

``` math
dA^{[l-1]} := \frac{\partial J }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \tag{6}
```

- Beachten Sie:

  - Der ForwÃ¤rtsschirtt Ã¼bernimmt $`A^{[l-1]}`$ von dem vorherigen
    Schritt und gibt $`A^{[l]}`$ an den nÃ¤chsten Schritt weiter.
  - Der RÃ¼ckwÃ¤rtschritt Ã¼bernimmt $`dA^{[l]}`$ von dem vorherigen
    Schritt und gibt $`dA^{[l-1]}`$ an den nÃ¤chsten RÃ¼ckwÃ¤rtsschritt
    weiter.

### Parameteraktualisierung

- Die Aktualisierung der Parameter in Schicht $`l`$ erfolgt wie gewohnt
  durch: Dabei bezeichnet $`\alpha`$ die Lernrate.

``` math
W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \tag{7}
```

``` math
b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]} \tag{8}
```

> [!NOTE]
>
> <details>
>
> <summary><strong>âœ… Lernziele</strong></summary>
>
> - k2: Ich kann den ForwÃ¤rts- und RÃ¼ckwÃ¤rtslauf in Matrix Notation mit
>   mehreren Datenpunkten als Eingabe erklÃ¤ren
> - k3: Ich kann Aktivierungsfunktionen ableiten
> - k3: Ich kann die Berechnung der partiellen Ableitungen durchfÃ¼hren
> - k3: Ich kann den RÃ¼ckwÃ¤rtslauf (backpropagation) fÃ¼r ein gegebenes
>   MLP durchfÃ¼hren
>
> </details>

> [!TIP]
>
> <details>
>
> <summary><strong>ðŸ§© Quizzes</strong></summary>
>
> - [Selbsttest Backpropagation
>   (ILIAS)](https://www.hsbi.de/elearning/goto.php?target=tst_1106593&client_id=FH-Bielefeld)
>
> </details>

------------------------------------------------------------------------

<img src="https://licensebuttons.net/l/by-sa/4.0/88x31.png" width="10%">

Unless otherwise noted, this work is licensed under CC BY-SA 4.0.

<blockquote><p><sup><sub><strong>Last modified:</strong> 578b7ff (lecture: fix links to attachments, 2025-11-17)<br></sub></sup></p></blockquote>
