# NN07: Training & Testing

> [!TIP]
>
> <details open>
>
> <summary><strong>ğŸ¦ Videos</strong></summary>
>
> - [NN7.1 - Training, Testing,
>   Validierung](https://youtu.be/PUw-TvLJULI)
> - [NN7.2 - Kreuzvalidierung](https://youtu.be/DqjdZ8HaDSo)
> - [NN7.3 - Beispiel](https://youtu.be/7XATTMNI-gI)
>
> </details>

> [!NOTE]
>
> <details open>
>
> <summary><strong>ğŸ–‡ Weitere Unterlagen</strong></summary>
>
> - [NN07-Testing-Validierung.pdf](https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/lecture/nn/files/NN07-Testing-Validierung.pdf)
>
> </details>

## Kurze Ãœbersicht

### Training und Testing

- Der tatsÃ¤chliche **Erfolg** eines Modells wird nicht durch niedrige
  Trainingskosten gemessen, sondern durch geringe Kosten auf ungesehenen
  Daten, d.h. **hohe Vorhersagekraft, gute Generalisierung**!

- Die Menge aller gelabelten Daten in **Trainingsset und Testset**
  aufteilen, Testset nicht wÃ¤hrend des Trainings einsetzen!.

  - $`E_{in}`$ bezeichnet den Fehler auf dem Trainingsset, auch
    **in-sample error**.
  - $`E_{out}`$ bezeichnet den Fehler auf dem gesamten Eingaberaum
    $`X`$, auch **out-of-sample error**. $`E_{out}`$ ist der eigentliche
    Indikator fÃ¼r den zukÃ¼nftigen Erfolg des Modells, ist uns aber nicht
    zugÃ¤nglich.
  - $`E_{test}`$ bezeichnet den Fehler auf dem Testset und ist eine
    **NÃ¤herung** fÃ¼r $`E_{out}`$.

> [!TIP]
>
> **Analogie**:
>
> $`E_{in}`$ : Erfolg in Ãœbungsaufgaben und ProbeprÃ¼fungen.
>
> $`E_{test}`$ : Erfolg in EndprÃ¼fung.

- Die NÃ¤herung $`E_{test}`$ sollte mÃ¶glichst genau sein, damit es als
  ein verlÃ¤ssliches **GÃ¼tesiegel** dienen kann.

  - Das Testset sollte genug Daten enthalten. Ãœblicher Anteil an
    Testdaten:
    - bei $`|D| \approx 100.000 \rightarrow`$ ca. 20%
    - bei $`|D| \approx 10.000.000 \rightarrow`$ ca. 1%
    - Beispiel: Hat man 1000 Beispiele im Testset, wird $`E_{test}`$ mit
      $`\ge 98\%`$ Wahrscheinlichkeit in der $`\pm 5\%`$ Umgebung von
      $`E_{out}`$ liegen (fÃ¼r theoretische Grundlagen und Herleitung
      siehe ([Abu-Mostafa u.Â a. 2012, 39â€“69](#ref-AbuMostafa2012))).
  - Trainingsdaten und Testdaten sollten mÃ¶glichst aus derselben
    Verteilung kommen, wie die zukÃ¼nftigen **Real-World-Daten**.

- **Wichtige Bemerkung**:

  - Testdaten nicht anfassen, bis das Modell Einsatzbereit ist!
  - Die Testdaten dÃ¼rfen in **keinster Weise** bei der Auswahl der
    endgÃ¼ltigen Hypothese eingesetzt werden, weder bei der Berechnung
    der Parameter (Training), noch bei der Bestimmung der Hyperparameter
    (Hyperparameter-Tuning).
  - Sobald der Testfehler die Auswahl der endgÃ¼ltigen Hypothese
    beeinflusst, kann sie nicht mehr als â€œGÃ¼tesiegelâ€ eingesetzt
    werden.  
    **CHECK**: HÃ¤tte man zufÃ¤llig andere Testdaten gewÃ¤hlt, kÃ¶nnte sich
    dadurch die endgÃ¼ltige Hypothese Ã¤ndern?

### Validierung und Modellauswahl

- Das Ziel ist es, das Modell mit bester Generalisierung, also kleinstem
  $`E_{out}`$ zu bestimmen. $`E_{out}`$ ist jedoch unbekannt und die
  NÃ¤herung $`E_{test}`$ *darf nicht* bei der Modellauswahl eingesetzt
  werden.

- LÃ–SUNG: Einen weiteren Teil der Daten als **Validierungsset** (auch
  *development set*) beiseitelegen und nicht fÃ¼r das Training
  (i.e.Â Minimierung des Trainingsfehlers $`E_{in}`$) verwenden!

- **Bemerkung**:

  Das Wort **Modell** kann je nach Kontext unterschiedliche Bedeutungen
  annehmen.  
  Ein Modell im aktuellen Kontext ist als ein Paar
  $`(\mathcal{H},\mathcal{A})`$ von Hypothesenraum (bzw.
  **Modellarchitektur**) und **Lernalgorithmus** definiert.

  - Die Auswahl eines Modells kann aus einer Menge von Modellen
    unterschiedlicher Art erfolgen (z.B. lineare Modelle, polynomiale
    Modelle, neuronale Netze), oder von Modellen derselben Art aber mit
    unterschiedlichen Hyperparametern (z.B. Neuronale Netze mit
    unterschiedlicher Anzahl von versteckten Schichten).
  - AuÃŸerdem kann dieselbe Modellarchitektur $`\mathcal{H}`$ mit
    unterschiedlichen Lernalgorithmen trainiert werden, was wiederum die
    endgÃ¼ltige Hypothese beeinflussen kann. Die Bestimmung der
    Hyperparameter von $`{\mathcal{A}}`$ (wie z.B. Optimierungsfunktion,
    Lernrate, Kostenfunktion, Regularisierungsparameter usw.) sind daher
    auch Teil der Modellauswahl.

- Der **Validierungsfehler $`E_{val}`$** kann nun als
  Entscheidungsgrundlage an verschiedenen Stellen des Lernrpozesses
  eingesetzt werden, wie zum Beispiel:

  - Bei der **Auswahl geeigneter Hyperparameter** wie z.B. Anzahl
    Schichten, Anzahl Zellen/Schicht, Aktivierungsfunktion,
    Regularisierungsparameter (siehe Abbildung 1).

  <p align="center"><picture><source media="(prefers-color-scheme: light)" srcset="images/val1_light.png"><source media="(prefers-color-scheme: dark)" srcset="images/val1_dark.png"><img src="images/val1.png" width="80%"></picture></p><p align="center">Abbildung
  1 - Einsatz der Validierung fÃ¼r das Hyperparameter-Tuning</p>

  - Bei der **Auswahl der endgÃ¼ltigen Hypothese** ($`\rightarrow`$
    Parameterauswahl!): unter allen Hypothesen, die wÃ¤hrend des
    Trainings durchlafen werden, wÃ¤hle jene mit kleinstem $`E_{val}`$
    (siehe Abbildung 2).

  <p align="center"><picture><source media="(prefers-color-scheme: light)" srcset="images/val2_light.png"><source media="(prefers-color-scheme: dark)" srcset="images/val2_dark.png"><img src="images/val2.png" width="80%"></picture></p><p align="center">Abbildung
  2 - Einsatz der Validierung bei der Auswahl der entgÃ¼ltigen
  Hypothese</p>

  - Bei der graphischen **Darstellung von Lernkurven** fÃ¼r die Diagnose
    von Ãœber- und Unteranpassung (siehe Abbildung 3).

  <p align="center"><picture><source media="(prefers-color-scheme: light)" srcset="images/val3_light.png"><source media="(prefers-color-scheme: dark)" srcset="images/val3_dark.png"><img src="images/val3.png" width="80%"></picture></p><p align="center">Abbildung
  3 - Lernkurven</p>

- Ãœbliche train/val/test Aufteilung der Daten (in Prozent):

  - bei $`|D| \approx 100.000 \rightarrow`$ ca. 60/20/20
  - bei $`|D| \approx 10.000.000 \rightarrow`$ ca. 98/1/1

- **Bemerkung**:

  Das Modell ist trainiert fÃ¼r gute Ergebnisse auf Trainingsdaten und
  â€œfine-tunedâ€ fÃ¼r gute Ergebnisse auf den Validierungsdaten. Ergebnisse
  auf Testdaten werden mit hoher wahrscheinlichkeit schlechter
  ausfallen, als auf Validierungsdaten ($`E_{val}`$ ist eine zu
  optimistische NÃ¤herung).

- Sind Validierungs- und/oder Trainingsset zu klein, fÃ¼hrt das zu
  schlechten NÃ¤herungen $`E_{val}`$ und folglich zu schlechten
  Entscheidungen.

  - Bei der Aufteilung muss ein gutes Trade-off gefunden werden.
  - Wenn kein GÃ¼tesiegel notwendig ist, kann man auf das Testset
    verzichten und die Daten in Trainings- und Validierungsset
    aufteilen.
  - FÃ¼r eine bessere NÃ¤herung mit weniger Validierungsdaten kann k-fache
    Kreuzvalidierung eingesetzt werden (wenn genug RechenkapazitÃ¤t
    vorhanden ist).

### K-fache Kreuzvalidierung (engl. k-fold cross-validation):

- Das Modell $`(\mathcal{H_m},\mathcal{A_m})`$ wird $`k`$ mal trainiert
  und validiert, jedes mal mit unterschiedlichen Trainings- und
  Validierungsmengen:

  - Die Trainingsdaten werden in $`k`$ disjunkte Teilmengen
    $`D_1, D_2, ..., D_k`$ aufgeteilt.

  - Bei dem $`i`$-ten Training werden die Teilmenge $`D_i`$ fÃ¼r die
    Berechnung des Validierungsfehlers $`e_i := E_{val}(h_m^{*(i)})`$
    und die restlichen $`k-1`$ Teilmengen fÃ¼r das Training verwendet.

  - Der **Kreuzvalidierungsfehler** des Modells
    $`(\mathcal{H_m},\mathcal{A_m})`$ ist der Durchschnitt der $`k`$
    Validierungsfehler $`e_1, e_2, ..., e_k`$ (siehe Abbildung 4).

  <p align="center"><picture><source media="(prefers-color-scheme: light)" srcset="images/val4_light.png"><source media="(prefers-color-scheme: dark)" srcset="images/val4_dark.png"><img src="images/val4.png" width="80%"></picture></p><p align="center">Abbildung
  4 - Kreuzvalidierung</p>

``` math
E_{CV}(m) := \frac{1}{k} \sum_{i=1}^{k} e_i = \frac{1}{k} \sum_{i=1}^{k} E_{val}(h_m^{*(i)})
```

- Bemerkung: Die Kreuzvalidierung wird nur bei der Modellauswahl
  eingesetzt: es liefert verlÃ¤sslichere NÃ¤herungen fÃ¼r $`E_{out}`$ und
  fÃ¼hrt daher zu besseren Entscheidungen. Das zuletzt ausgewÃ¤hlte Modell
  wird danach wie gewohnt auf den gesamten Trainigsdaten (ausgenommen
  Testdaten) trainiert und zum Schluss mit den Testdaten evaluiert.

> [!NOTE]
>
> <details>
>
> <summary><strong>âœ… Lernziele</strong></summary>
>
> - k2: Ich kann die Begriffe Trainings-, Validierungs- und Testfehler
>   erklÃ¤ren
> - k2: Ich kann den Zweck einer Testmenge erlÃ¤utern
> - k2: Ich kann das Verfahren der Kreuzvalidierung erklÃ¤ren
> - k2: Ich kann den Begriff Hyperparameter-Tuning erklÃ¤ren
> - k3: Ich kann verschiedene Lernkurven interpretieren
>
> </details>

> [!TIP]
>
> <details>
>
> <summary><strong>ğŸ§© Quizzes</strong></summary>
>
> - [Selbsttest Training & Testing
>   (ILIAS)](https://www.hsbi.de/elearning/goto.php?target=tst_1106594&client_id=FH-Bielefeld)
>
> </details>

------------------------------------------------------------------------

> [!NOTE]
>
> <details>
>
> <summary><strong>ğŸ‘€ Quellen</strong></summary>
>
> <div id="refs" class="references csl-bib-body hanging-indent">
>
> <div id="ref-AbuMostafa2012" class="csl-entry">
>
> Abu-Mostafa, Y. S., M. Magdon-Ismail, und H. Lin. 2012. *Learning From
> Data*. AMLBook. <https://work.caltech.edu/telecourse>.
>
> </div>
>
> </div>
>
> </details>

------------------------------------------------------------------------

<img src="https://licensebuttons.net/l/by-sa/4.0/88x31.png" width="10%">

Unless otherwise noted, this work is licensed under CC BY-SA 4.0.

<blockquote><p><sup><sub><strong>Last modified:</strong> 578b7ff (lecture: fix links to attachments, 2025-11-17)<br></sub></sup></p></blockquote>
