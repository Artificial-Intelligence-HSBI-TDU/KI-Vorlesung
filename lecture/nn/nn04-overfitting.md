# NN04: Overfitting und Regularisierung

> [!TIP]
>
> <details open>
>
> <summary><strong>ðŸŽ¦ Videos</strong></summary>
>
> - [NN4.1 - Nichtlineare Modelle](https://youtu.be/KJLT-h_ChRo)
> - [NN4.2 - Overfitting und
>   Regularisierung](https://youtu.be/BW91MYPUH_k)
>
> </details>

> [!NOTE]
>
> <details open>
>
> <summary><strong>ðŸ–‡ Weitere Unterlagen</strong></summary>
>
> - [NN04-Nichtlineare_Modelle_und_Overfitting.pdf](https://github.com/Artificial-Intelligence-HSBI-TDU/KI-Vorlesung/blob/master/lecture/nn/files/NN04-Nichtlineare_Modelle_und_Overfitting.pdf)
>
> </details>

## Kurze Ãœbersicht

### Nichtlineare Modelle

- EinfÃ¼hrung von neuen Merkmalen in Form von nichtlienaren Kombinationen
  der ursprÃ¼nglichen Merkmale
- ErhÃ¶hung der KomplexitÃ¤t des Modells ermÃ¶glicht das Erfassen von
  nichtlinearen Beziehungen
- **Bemerkung**: Die Hypothesenfunktion bleibt linear in den Gewichten,
  es wird weiterhin logistische Regression in einem **erweiterten**
  Merkmalraum durchgefÃ¼hrt.

### Ãœberanpassung und Regularisierung

- Die **Ãœberanpassung** (engl. Overfitting) ist eines der hÃ¤ufigsten und
  wichtigsten Probleme in ML und DL

  > â€œWas im Bereich des maschinellen Lernens Professionelle von
  > Amateuren unterscheidet, ist ihre FÃ¤higkeit mit Ãœberanpassung
  > umzugehen.â€
  >
  > â€“ Quelle: ([Abu-Mostafa u.Â a. 2012, 119](#ref-AbuMostafa2012))

- Anzeichen von Ãœberanpassung sind geringe Trainingskosten und hohe
  **Testkosten** (Kosten auf nicht-gesehenen Daten).

- Regularisierung ist eine MaÃŸnahme gegen Ãœberanpassung. Man kann es
  sich als eine Reduktion in der KomplexitÃ¤t des Modells vorstellen.

- Der Regularisierungsparameter $`\lambda`$ ist ein Hyperparameter. Je
  grÃ¶ÃŸer der $`\lambda`$-Wert, desto grÃ¶ÃŸer der Regularisierungseffekt.

- Die **Kostenfunktion** bei regulariserter logistischer Regression:

``` math
J = \frac{1}{m} \left\lbrack \sum_{i=1}^m \left( -y^{[i]}log(a^{[i]})-(1-y^{[i]})log(1-a^{[i]}) \right) + \frac{\lambda}{2} \sum_{j=1}^n (w^2_j)  \right\rbrack \tag{1}
```

- Die **Gewichtsaktualisierung** mit Regularisierungsterm:

``` math
w_j := w_j - \frac{\alpha}{m} \left\lbrack \sum_{i=1}^m \left( ( a^{[i]} - y^{[i]} )x_j^{[i]} \right) + \lambda w_j  \right\rbrack \tag{2}
```

## ðŸ“– Zum Nachlesen

- Abu-Mostafa u.Â a. ([2012](#ref-AbuMostafa2012)): Kapitel 4

> [!NOTE]
>
> <details>
>
> <summary><strong>âœ… Lernziele</strong></summary>
>
> - k2: Ich kann die ErhÃ¶hung der Modell-KomplexitÃ¤t durch EinfÃ¼hrung
>   von Merkmalen hÃ¶herer Ordnung erklÃ¤ren
> - k2: Ich kann die Begriffe Unter- und Ãœberanpassung an einem Beispiel
>   erklÃ¤ren
> - k2: Ich kann den Begriff Regularisierung erklÃ¤ren und die Auswirkung
>   auf die Gewichte und das Modell erlÃ¤utern
> - k3: Ich kann den Gradientenabstieg fÃ¼r regularisierte logistische
>   Regression an einem Beispiel anwenden
>
> </details>

> [!TIP]
>
> <details>
>
> <summary><strong>ðŸ§© Quizzes</strong></summary>
>
> - [Selbsttest Overfitting
>   (ILIAS)](https://www.hsbi.de/elearning/goto.php?target=tst_1106595&client_id=FH-Bielefeld)
>
> </details>

------------------------------------------------------------------------

> [!NOTE]
>
> <details>
>
> <summary><strong>ðŸ‘€ Quellen</strong></summary>
>
> <div id="refs" class="references csl-bib-body hanging-indent">
>
> <div id="ref-AbuMostafa2012" class="csl-entry">
>
> Abu-Mostafa, Y. S., M. Magdon-Ismail, und H. Lin. 2012. *Learning From
> Data*. AMLBook. <https://work.caltech.edu/telecourse>.
>
> </div>
>
> </div>
>
> </details>

------------------------------------------------------------------------

<img src="https://licensebuttons.net/l/by-sa/4.0/88x31.png" width="10%">

Unless otherwise noted, this work is licensed under CC BY-SA 4.0.

<blockquote><p><sup><sub><strong>Last modified:</strong> 578b7ff (lecture: fix links to attachments, 2025-11-17)<br></sub></sup></p></blockquote>
