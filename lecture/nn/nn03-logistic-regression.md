# NN03: Logistische Regression

> [!TIP]
>
> <details open>
>
> <summary><strong>ðŸŽ¦ Videos</strong></summary>
>
> - [NN3.1 - Logistische Regression -
>   Intro](https://youtu.be/GpJmjrqA5RY)
> - [NN3.2 - Logistische Regression - Hypothesenfunktion und
>   Bsp](https://youtu.be/z-jFZeNWMRc)
> - [NN3.3 - Logistische Regression - Verlust und
>   Kosten](https://youtu.be/ruuCKupOhCE)
> - [NN3.4 - Logistische Regression -
>   Gradientenabstieg](https://youtu.be/kPAZsr-r1LA)
>
> </details>

> [!NOTE]
>
> <details open>
>
> <summary><strong>ðŸ–‡ Weitere Unterlagen</strong></summary>
>
> - [NN03-Logistische_Regression.pdf](files/NN03-Logistische_Regression.pdf)
>
> </details>

## Kurze Ãœbersicht

### Formalisierung

- Ausgabe $`y`$ ist reelle Zahl aus dem stetigen Bereich $`(0,1)`$

- Die **Hypothesenfunktion** ist:

``` math
h(\mathbf{x}) = \sigma (\mathbf{w}^T\mathbf{x}) = \sigma (w_0 + w_1x_1 + w_2x_2 + \ldots + w_nx_n) \tag{1}
```

- Der **Kreuzentropie Verlust** (engl. Cross-Entropy) fÃ¼r einen
  Datenpunkt $`\mathbf{x}`$: wobei hier $`a := \hat{y}`$ die Vorhersage
  ist.

``` math
\mathcal{L}(a, y) =  - y  \log(a) - (1-y)  \log(1-a)\tag{2}
```

- Die Kosten als durchschnittlicher Verlust Ã¼ber alle Datenpunkte
  $`x^{(1)}, \ldots, x^{(m)}`$:

``` math
J = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(a^{(i)}, y^{(i)})\tag{3}
```

### Gradientenabstieg

- Der Gradient fÃ¼r einen Datenpunkt $`\mathbf{x}`$:

``` math
\frac{\partial \mathcal{L}}{\partial w} = (a-y)x \tag{4}
```

- Der Gradient fÃ¼r alle Datenpunkte $`X`$ in Matrix-Notation:

``` math
\nabla J = \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{5}
```

### Graphische Ãœbersicht

- **Logistische Regression**

  <picture><source media="(prefers-color-scheme: light)" srcset="images/log_reg_nn_light.png"><source media="(prefers-color-scheme: dark)" srcset="images/log_reg_nn_dark.png"><img src="images/log_reg_nn.png" width="65%"></picture>

- **Lineare Regression**

  <picture><source media="(prefers-color-scheme: light)" srcset="images/lin_reg_nn_light.png"><source media="(prefers-color-scheme: dark)" srcset="images/lin_reg_nn_dark.png"><img src="images/lin_reg_nn.png" width="65%"></picture>

- **Perzeptron**

  <picture><source media="(prefers-color-scheme: light)" srcset="images/perzeptron_nn_light.png"><source media="(prefers-color-scheme: dark)" srcset="images/perzeptron_nn_dark.png"><img src="images/perzeptron_nn.png" width="65%"></picture>

> [!NOTE]
>
> <details>
>
> <summary><strong>âœ… Lernziele</strong></summary>
>
> - k2: Ich kann die logistische Regression aus Sicht neuronaler Netze
>   erklÃ¤ren und dabei auf die graphische Darstellung und den Vergleich
>   mit Perzeptron und linearer Regression eingehen
> - k2: Ich kann die Formalisierung der logistische Regression erklÃ¤ren
> - k2: Ich kann die Sigmoid-Aktivierungsfunktion erklÃ¤ren und an einem
>   Beispiel demonstrieren
> - k2: Ich kann Verlust- und Kosten (Cross-Entropy Loss) erklÃ¤ren
> - k3: Ich kann den Gradientenabstieg fÃ¼r logistische Regression an
>   einem Beispiel anwenden
>
> </details>

> [!TIP]
>
> <details>
>
> <summary><strong>ðŸ§© Quizzes</strong></summary>
>
> - [Selbsttest Logistische Regression
>   (ILIAS)](https://www.hsbi.de/elearning/goto.php?target=tst_1106591&client_id=FH-Bielefeld)
>
> </details>

------------------------------------------------------------------------

<img src="https://licensebuttons.net/l/by-sa/4.0/88x31.png" width="10%">

Unless otherwise noted, this work is licensed under CC BY-SA 4.0.

<blockquote><p><sup><sub><strong>Last modified:</strong> 9c1a02d (lecture: use local files for attachments (NN3), 2025-10-15)<br></sub></sup></p></blockquote>
