# NB: Klassifikation mit Naive Bayes

> [!IMPORTANT]
>
> <details open>
>
> <summary><strong>ğŸ¯ TL;DR</strong></summary>
>
> Mit Hilfe der (verallgemeinerten) Bayes-Regel kann man Klassifikation
> durchfÃ¼hren. Dazu werden beim â€œTrainingâ€ die bedingten
> Wahrscheinlichkeiten aus den Trainingsdaten geschÃ¤tzt. Die Anwendung
> (Klassifikation) erfolgt dann durch die Nutzung der beim â€œTrainingâ€
> berechneten bedingten Wahrscheinlichkeiten:
>
> $`h_{MAP} = \mathop{\text{argmax}}_{h \in H} \: P(h \mid D_1,  \ldots, D_n) =
> \mathop{\text{argmax}}_{h \in H} \: P(h) \prod_i P(D_i \mid h)`$
>
> FÃ¼r jede Hypothese $`h`$, d.h. fÃ¼r jede Klasse, wird der Posterior
> $`P(h \mid D_1, \ldots, D_n)`$ ausgerechnet. Die Klasse, deren Wert
> dabei am hÃ¶chsten ist, â€œgewinntâ€, d.h. die Klasse mit dem grÃ¶ÃŸten
> Posterior wird ausgegeben. (Deshalb wird das Verfahren oft auch â€œMAPâ€
> â€“ *Maximum a Posteriori* â€“ genannt.)
>
> Bei der Berechnung wird angenommen, dass die betrachteten Merkmale
> (bedingt) unabhÃ¤ngig sind (dies geht in die obige Formel ein). Diese
> Annahme trifft aber oft nicht zu, deshalb auch der Name â€œ*Naive* Bayes
> Klassifikationâ€. Man berechnet in diesem Fall falsche Werte. Dennoch
> zeigt der Algorithmus in der Praxis sehr gute Ergebnisse.
>
> Durch den Einsatz der bedingten Wahrscheinlichkeiten in der
> Produktformel ergeben sich einige Schwierigkeiten:
>
> 1.  Wenn beim â€œTrainingâ€ AusprÃ¤gungen fehlen, ist die bedingte
>     Wahrscheinlichkeit Null. Dadurch wird das gesamte Produkt Null.
>     Zur Abhilfe kann man den **Laplace-SchÃ¤tzer** nutzen, der
>     (gesteuert Ã¼ber einen Parameter) gewissermaÃŸen virtuelle
>     Trainingsbeispiele beisteuert.
> 2.  Durch das Produkt vieler kleiner Werte kann es schnell zu
>     *Floating Point*-Underflows kommen. Hier kann man einen Trick
>     nutzen: Man berechnet den Logarithmus der Produktformel. Dadurch
>     Ã¤ndern sich zwar die absoluten Werte, die Reihenfolge der
>     Hypothesen bleibt aber erhalten. Da wir nur nach der Hypothese
>     suchen, die einen hÃ¶heren Wert als die anderen hat, und nicht den
>     absoluten Wert an sich benÃ¶tigen, kann man so vorgehen. Durch den
>     Logarithmus wird aus dem Produkt eine Summe, wo die kleinen Werte
>     der bedingten Wahrscheinlichkeiten nicht so starke Auswirkungen
>     haben wie im Produkt.
>
> Oft nimmt man zusÃ¤tzlich an, dass fÃ¼r alle Hypothesen (Klassen) $`h`$
> der Prior $`P(h)`$ gleich ist. Dann kann man diesen Faktor ebenfalls
> aus der Berechnung entfernen. Dieses Verfahren nennt man auch
> â€œ**Maximum Likelihood**â€.
>
> Der NB-Klassifikator wird gern fÃ¼r die Textklassifikation eingesetzt.
> Hier muss man einem Text ein Label zuordnen. In einer Vorverarbeitung
> wird zunÃ¤chst eine Menge der relevanten WÃ¶rter Ã¼ber alle
> Trainingstexte gebildet (*Bag-of-Words*). Der Bag-of-Words entspricht
> einem Merkmalsvektor, wobei die Merkmale die einzelnen WÃ¶rter sind.
> Dann kann jeder Text der Trainingsmenge Ã¼ber so einen Merkmalsvektor
> dargestellt werden: Entweder man gibt pro Merkmal an, ob es da (1)
> oder nicht da (0) ist oder man zÃ¤hlt die HÃ¤ufigkeit des Auftretens.
> Dann kann man mit dem NB-Klassifikator die bedingten
> Wahrscheinlichkeiten schÃ¤tzen und einen neuen Text klassifizieren.
> </details>

> [!TIP]
>
> <details open>
>
> <summary><strong>ğŸ¦ Videos</strong></summary>
>
> - [VL Naive Bayes Klassifikation](https://youtu.be/qfX4zp1i-Co)
>
> </details>

## Medizinische Diagnostik mit NB

- Bei Arthrose wird in 80 Prozent der FÃ¤lle ein steifes Gelenk
  beobachtet: $`P(S \mid A) = 0.8`$
- Eine von 10.000 Personen hat Arthrose: $`P(A) = 0.0001`$
- Eine von 10 Personen hat ein steifes Gelenk: $`P(S) = 0.1`$

=\> Ich habe ein steifes Gelenk. Habe ich Arthrose?

## Textklassifikation mit NB

- Mails, manuell markiert:
  - D1: (â€œSieben Zwerge fraÃŸen sieben Ziegenâ€, OK)
  - D2: (â€œSieben Ziegen traten sieben WÃ¶lfeâ€, SPAM)
  - D3: (â€œSieben WÃ¶lfe fraÃŸen sieben BÃ¶ckeâ€, OK)
  - D4: (â€œSieben BÃ¶cke traten sieben Zwergeâ€, SPAM)

<!-- -->

- Neue Mails:
  - T1: (â€œSieben Zwerge fraÃŸen sieben WÃ¶lfeâ€)
  - T2: (â€œSieben Zwerge traten sieben Ziegenâ€)

Lernen Sie mit Hilfe der Trainingsmenge einen Naive-Bayes-Klassifikator
und wenden Sie diesen auf die beiden Test-Dokumente an.

## Naive Bayes

- Verallgemeinerte Bayes Regel

``` math
P(H \mid D_1, \ldots, D_n) = \frac{P(D_1, \ldots, D_n \mid H)P(H)}{P(D_1, \ldots, D_n)}
```

- Annahme: $`D_i`$ sind bedingt unabhÃ¤ngig

``` math
P(D_1, \ldots, D_n \mid H) = P(D_1 \mid H) \cdot \ldots \cdot P(D_n \mid H) = \prod_i P(D_i \mid H)
```

- Beobachtung: $`P(D_1, \ldots, D_n)`$ fÃ¼r alle Hypothesen $`h \in H`$
  gleich

<!-- -->

- **Naive Bayes Klassifikator** bzw. **MAP** (â€œMaximum a Posterioriâ€)

  Naive Bayes: WÃ¤hle die plausibelste Hypothese, die von den Daten
  unterstÃ¼tzt wird.

``` math
h_{MAP} = \mathop{\text{argmax}}_{h \in H} \: P(h \mid D_1, \ldots, D_n)
= \mathop{\text{argmax}}_{h \in H} \: P(h) \prod_i P(D_i \mid h)
```

## Bayesâ€™sches Lernen

``` math
h_{MAP} = \mathop{\text{argmax}}_{h \in H} \: P(h \mid D_1, \ldots, D_n)
= \mathop{\text{argmax}}_{h \in H} \: P(h) \prod_i P(D_i \mid h)
```

**Training**: Bestimme die Wahrscheinlichkeiten aus Trainingsdaten
$`\mathbf{S}`$

- FÃ¼r jede Klasse $`h`$:
  - SchÃ¤tze $`P(h) = \dfrac{\lvert S(h) \rvert}{\lvert S \rvert}`$
  - FÃ¼r jedes Attribut $`D_i`$ und jede AusprÃ¤gung $`x \in D_i`$:
    SchÃ¤tze
    $`P(D_i=x \mid h) = \dfrac{\lvert S_{D_i}(x) \cap S(h) \rvert}{\lvert S(h) \rvert}`$

**Klassifikation**: WÃ¤hle wahrscheinlichste Klasse $`h_{MAP}`$ fÃ¼r
Vektor $`\mathbf{x}`$

- $`h_{MAP} = \mathop{\text{argmax}}_{h \in H} \: P(h) \prod_{x \in \mathbf{x}} P(x \mid h)`$

## Beispiel Klassifikation mit NB

| Nase lÃ¤uft | Husten | GerÃ¶tete Haut | Fieber | Klasse |
|------------|--------|---------------|--------|--------|
| 1          | 1      | 1             | 0      | krank  |
| 1          | 1      | 0             | 0      | krank  |
| 0          | 0      | 1             | 1      | krank  |
| 1          | 0      | 0             | 0      | gesund |
| 0          | 0      | 0             | 0      | gesund |

- Eingabe: Person mit Husten und Fieber

Gesucht: $`P(\text{krank})`$, $`P(\text{gesund})`$,
$`P(\text{Nase=0} \mid \text{krank})`$,
$`P(\text{Nase=0} \mid \text{gesund})`$, â€¦

WÃ¤hle Klasse
``` math
\begin{eqnarray}
h_{MAP} = \mathop{\text{argmax}}_{h \in \lbrace \text{gesund, krank} \rbrace} & P(h) \cdot P(\text{Nase=0} \mid h) \cdot P(\text{Husten=1} \mid h) \\
    & \cdot P(\text{Haut=0} \mid h) \cdot P(\text{Fieber=1} \mid h)
\end{eqnarray}
```

**Ergebnis**: (nur die fÃ¼r den zu klassifizierenden Beispiel-Vektor
nÃ¶tigen Werte, die restlichen mÃ¼ssten aber auch beim â€œTrainingâ€
berechnet werden!)

    P(gesund) = 2/5 = 0.4
    P(krank)  = 3/5 = 0.6

    P(Nase=0 | gesund) = 1/2 = 0.5
    P(Nase=0 | krank)  = 1/3 = 0.333

    P(Husten=1 | gesund) = 0/2 = 0
    P(Husten=1 | krank)  = 2/3 = 0.667

    P(Haut=0 | gesund) = 2/2 = 1
    P(Haut=0 | krank)  = 1/3 = 0.333

    P(Fieber=1 | gesund) = 0/2 = 0
    P(Fieber=1 | krank)  = 1/3 = 0.333

    h = gesund: P(gesund) * P(Nase=0 | gesund) * P(Husten=1 | gesund) * P(Haut=0 | gesund) * P(Fieber=1 | gesund) = 0.4*0.5*0*1*0              = 0
    h = krank:  P(krank)  * P(Nase=0 | krank)  * P(Husten=1 | krank)  * P(Haut=0 | krank)  * P(Fieber=1 | krank)  = 0.6*0.333*0.667*0.33*0.333 = 0.015

=\> Klasse â€œkrankâ€ gewinnt (Wert fÃ¼r $`P(\text{krank})`$ ist der hÃ¶chste
der beiden Hypothesen) â€¦

## Textklassifikation mit NB

- Texte als Trainingsmenge:
  - Text zerlegen in Terme (WÃ¶rter, sonstige relevante Token)
  - ggf. Entfernen von StoppwÃ¶rtern (beispielsweise Artikel u.Ã¤.)
  - ggf. Stemming und Lemmatisierung fÃ¼r restliche Terme
  - ggf. weitere Vorverarbeitungsschritte (GroÃŸ-Klein-Schreibung, â€¦)
  - Terme zusammenfassen als Menge: *â€œBag of Wordsâ€* (mit HÃ¤ufigkeit)

<!-- -->

- Naive Bayes â€œtrainierenâ€:
  - A-priori-Wahrscheinlichkeit der Klassen:
    $`P(c) = \dfrac{N_c}{N} = \dfrac{\text{Anzahl Dokumente in Klasse c}}{\text{Anzahl Dokumente}}`$

  <!-- -->

  - Likelihood der Daten (Terme):
    - $`P(t \mid c) = \dfrac{\mathop{\text{count}}(t,c)}{\sum_{v \in V} \mathop{\text{count}}(v,c)}`$
      mit $`\mathop{\text{count}}(t,c)`$ Anzahl der Vorkommen von Term
      $`t`$ in allen Dokumenten der Klasse $`c`$ und $`V`$ die
      Vereinigung aller Terme aller Dokumente (als Menge)

    <!-- -->

    - Variante mit Laplace-GlÃ¤ttung (s.u.):
      $`P(t \mid c) = \dfrac{\mathop{\text{count}}(t,c) + 1}{\sum_{v \in V} \mathop{\text{count}}(v,c) + \lvert V \rvert}`$

## NaivitÃ¤t im Naive Bayes

- UnabhÃ¤ngigkeit der Attribute oft nicht gegeben

  =\> $`P(D_1, \ldots, D_n \mid H) \ne \prod_i P(D_i \mid H)`$

- A-posteriori-Wahrscheinlichkeiten oft unrealistisch nah an 1 oder 0

<!-- -->

- Praxis: Dennoch hÃ¤ufig sehr gute Ergebnisse

  Wichtig: Solange die **Maximierung** Ã¼ber alle Hypothesen die selben
  Ergebnisse liefert, mÃ¼ssen die konkreten SchÃ¤tzungen/Werte nicht exakt
  stimmen â€¦

Wenn Attribute nicht (bedingt) unabhÃ¤ngig sind, kann sich der NB
verschÃ¤tzen, d.h. es kommt dann u.U. zu einer hÃ¶heren Fehlerrate, da
bestimmte Eigenschaften in der Trainingsmenge zu hoch gewichtet werden.

> [!TIP]
>
> ### Beispiel
>
> #### Gegebene Daten
>
> Seien die beiden Merkmale $`x_1`$ und $`x_2`$ mit den folgenden
> Verteilungen gegeben:
>
> - Klassen: $`H \in \lbrace 0, 1 \rbrace`$,
>   $`P(H = 0) = P(H = 1) = 0.5`$
> - Bedingte Verteilungen $`P(x_1, x_2 \mid H)`$:
>   - FÃ¼r $`H = 0`$:
>     - $`P(x_1=0, x_2=0 \mid 0) = 0.30`$
>     - $`P(x_1=0, x_2=1 \mid 0) = 0.35`$
>     - $`P(x_1=1, x_2=0 \mid 0) = 0.15`$
>     - $`P(x_1=1, x_2=1 \mid 0) = 0.20`$
>   - FÃ¼r $`H = 1`$:
>     - $`P(x_1=0, x_2=0 \mid 1) = 0.00`$
>     - $`P(x_1=0, x_2=1 \mid 1) = 0.30`$
>     - $`P(x_1=1, x_2=0 \mid 1) = 0.65`$
>     - $`P(x_1=1, x_2=1 \mid 1) = 0.05`$
>
> #### Analyse der gegebenen Daten
>
> Die Merkmale $`x_1`$ und $`x_2`$ sind *nicht* bedingt abhÃ¤ngig gegeben
> $`H`$.
>
> Erinnerung: Zwei Ereignisse $`X`$ und $`Y`$ sind bedingt unabhÃ¤ngig
> gegeben $`Z`$, wenn gilt
> $`P(X,Y \mid Z) = P(X \mid Y,Z)P(Y \mid Z) = P(X \mid Z)P(Y \mid Z)`$.
>
> Wir haben aber im Fall von $`H=1`$:
>
> - $`P(x_1=0, x_2=0 \mid 1) = 0.00`$
>   vs.Â $`P(x_1=0 \mid 1) P(x_2=0 \mid 1) = (0.00+0.30) * (0.00+0.65) = 0.30 * 0.65 = 0.195`$
> - $`P(x_1=0, x_2=1 \mid 1) = 0.30`$
>   vs.Â $`P(x_1=0 \mid 1) P(x_2=1 \mid 1) = (0.00+0.30) * (0.30+0.05) = 0.30 * 0.35 = 0.105`$
> - $`P(x_1=1, x_2=0 \mid 1) = 0.65`$
>   vs.Â $`P(x_1=1 \mid 1) P(x_2=0 \mid 1) = (0.65+0.05) * (0.00+0.65) = 0.70 * 0.65 = 0.455`$
> - $`P(x_1=1, x_2=1 \mid 1) = 0.05`$
>   vs.Â $`P(x_1=1 \mid 1) P(x_2=1 \mid 1) = (0.65+0.05) * (0.30+0.05) = 0.70 * 0.35 = 0.245`$
>
> (analog fÃ¼r $`H=0`$)
>
> Damit bekommen wir im Naive Bayes Klassifikator ein Problem. Dort wird
> von bedingt unabhÃ¤ngigen Merkmalen ausgegangen und deshalb die
> Vereinfachung von $`P(x_1, x_2 \mid H)`$ zu
> $`P(x_1 \mid H) P(x_2 \mid H)`$ vorgenommen. Da die Annahme nicht
> stimmt, werden die Merkmale falsch gewichtet und es kann zu
> Fehlklassifikationen kommen.
>
> #### Klassifikation einer Beobachtung
>
> Wir machen nun die folgende Beobachtung: $`X = (x_1=1, x_2=1)`$.
>
> Die nÃ¶tigen Marginalisierungen aus den Trainingsdaten fÃ¼r diese
> Beobachtung sind:
>
> - FÃ¼r $`H = 0`$: $`P(x_1=1 \mid 0) = 0.15 + 0.20 = 0.35`$,
>   $`P(x_2=1 \mid 0) = 0.35 + 0.20 = 0.55`$
> - FÃ¼r $`H = 1`$: $`P(x_1=1 \mid 1) = 0.65 + 0.05 = 0.70`$,
>   $`P(x_2=1 \mid 1) = 0.30 + 0.05 = 0.35`$
>
> Anwendung der Naive Bayes Klassifikation (mit Annahme bedingt
> unabhÃ¤ngige Merkmale): Wir nutzen
> $`h_{MAP} = \mathop{\text{argmax}}_{h \in H} \: P(h \mid D_1, \ldots, D_n)
> = \mathop{\text{argmax}}_{h \in H} \: P(h) \prod_i P(D_i \mid h)`$ und
> setzen unsere beiden Merkmale ein:
> $`h_{MAP} = \mathop{\text{argmax}}_{h \in H} \: P(h \mid x_1, x_2)
> = \mathop{\text{argmax}}_{h \in H} \: P(h) P(x_1 \mid h) P(x_2 \mid h)`$.
>
> Damit bekommen wir folgende Entscheidung:
>
> - $`H=0: 0.5 * 0.35 * 0.55 = 0.09625`$
> - $`H=1: 0.5 * 0.70 * 0.35 = 0.1225`$
> - Entscheidung fÃ¼r Klasse $`H=1`$
>
> Da die Merkmale nicht unabhÃ¤ngig sind, darf die Produktannahme nicht
> verwendet werden, sondern wir mÃ¼ssten eigentlich den Term
> $`P(x_1, x_2 \mid h)`$ nutzen:
> $`h_{MAP} = \mathop{\text{argmax}}_{h \in H} \: P(h \mid x_1, x_2)
> = \mathop{\text{argmax}}_{h \in H} \: P(h) P(x_1, x_2 \mid h)`$.
>
> Aus den gegebenen Daten haben wir (einfach oben ablesen):
>
> - $`P(x_1=1, x_2=1 \mid 0) = 0.20`$
> - $`P(x_1=1, x_2=1 \mid 1) = 0.05`$
>
> Eingesetzt in die Formel
> $`h_{MAP} = \mathop{\text{argmax}}_{h \in H} \: P(h \mid x_1, x_2)
> = \mathop{\text{argmax}}_{h \in H} \: P(h) P(x_1, x_2 \mid h)`$:
>
> - $`H=0: 0.5 * 0.20 = 0.10`$
> - $`H=1: 0.5 * 0.05 = 0.025`$
> - Entscheidung fÃ¼r Klasse $`H=0`$
>
> D.h. der Naive Bayes Klassifikator wÃ¼rde hier $`H=1`$ vorschlagen,
> wÃ¤hrend der korrekte Posterior fÃ¼r $`H=0`$ spricht.
>
> #### Interpretation
>
> In diesem konstruierten Beispiel fÃ¼hrt die AbhÃ¤ngigkeit der Merkmale
> zu einer Fehlkalibrierung der Posterior-Wahrscheinlichkeiten durch NB
> und damit zu einer falschen Klassifikation. In der RealitÃ¤t erweist
> sich der NB trotzdem relativ robust gegenÃ¼ber der AbhÃ¤ngigkeit von
> Merkmalen: Durch die Korrelation kommen hÃ¤ufig
> A-posteriori-Wahrscheinlichkeiten nahe 0 oder nahe 1 heraus, aber da
> es nur auf das Maximum und nicht auf den konkreten Wert ankommt,
> erhÃ¤lt man hÃ¤ufig trotzdem noch eine korrekte Klassifikation.

## Laplace-SchÃ¤tzer

- Problem: Attribut-AusprÃ¤gung fÃ¼r bestimmte Klasse nicht in
  Trainingsmenge:
  - =\> Bedingte Wahrscheinlichkeit ist 0
  - =\> Produkt gleich 0

<!-- -->

- LÃ¶sung: â€œLaplace-SchÃ¤tzerâ€ (auch â€œLaplace-GlÃ¤ttungâ€)

  Statt
  $`P(D_i=x \mid h) = \dfrac{\lvert S_{D_i}(x) \cap S(h) \rvert}{\lvert S(h) \rvert}`$

  nutze
  $`P(D_i=x \mid h) = \dfrac{\lvert S_{D_i}(x) \cap S(h) \rvert + m \cdot p_i}{\lvert S(h) \rvert + m}`$

  - mit $`m`$: frei wÃ¤hlbarer Faktor, und

  - $`p_i`$: A-priori-Wahrscheinlichkeit fÃ¼r $`P(D_i=x \mid h)`$

    Hintergrundwissen oder einfach *uniforme Verteilung der
    Attributwerte*: $`p_i = 1/\lvert D_i \rvert`$ (Wahrscheinlichkeit
    fÃ¼r eine AttributausprÃ¤gung ist 1/(Anzahl der AusprÃ¤gungen des
    Attributs))

  =\> â€œvirtuelleâ€ Trainingsbeispiele ($`m`$ ist die Zahl der virtuellen
  Trainingsbeispiele)

## Probleme mit Floating Point Underflow

- MAP berechnet Produkt mit vielen Termen
- Problem: Bei kleinen Zahlen kann **Floating Point Underflow**
  auftreten!

<!-- -->

- LÃ¶sung: Logarithmus maximieren (Produkt geht in Summe Ã¼ber)

  Erinnerung: $`\log(x \cdot y) = \log(x) + \log(y)`$ und Logarithmus
  streng monoton

``` math
\begin{eqnarray}
h_{MAP} &=& \mathop{\text{argmax}}_{h \in H} \: P(h \mid D_1, \ldots, D_n) \\[5pt]
        &=& \mathop{\text{argmax}}_{h \in H} \: P(h) \prod_i P(D_i \mid h) \\[5pt]
        &=& \mathop{\text{argmax}}_{h \in H} \: [\log(P(h)) + \sum_i \log(P(D_i \mid h))]
\end{eqnarray}
```

## Maximum Likelihood

- **Maximum a Posteriori**

``` math
h_{MAP} = \mathop{\text{argmax}}_{h \in H} \: P(h \mid D_1, \ldots, D_n)
= \mathop{\text{argmax}}_{h \in H} \: P(h) \prod_i P(D_i \mid h)
```

- Annahme: Klassen uniform verteilt =\> $`P(h_i) = P(h_j)`$

  **Maximum Likelihood**

  =\> Maximiere die Likelihood der Daten

``` math
h_{ML} = \mathop{\text{argmax}}_{h \in H} \: \prod_i P(D_i \mid h)
```

## Ausblick: Kontinuierliche Attribute

Bisher sind wir von diskreten Attributen ausgegangen. Bei
kontinuierlichen Attributen hat man zwei MÃ¶glichkeiten:

- Diskretisierung der Attribute: Aufteilung in Intervalle und
  Bezeichnung der Intervalle mit einem Namen
- Einsatz einer Verteilungsannahme und deren Dichtefunktion,
  beispielsweise Annahme von **normalverteilten** Daten mit der
  Dichtefunktion wobei $`\mu`$ der Mittelwert und $`\sigma^2`$ die
  Varianz der Daten sind.

``` math
f(x) = \frac{1}{\sqrt{2 \pi \sigma}} e^{- \frac{(x - \mu)^2}{2 \sigma^2}}
```

## Hinweis zum Sprachgebrauch

In AbhÃ¤ngigkeit von der Verteilung der $`P(D_i \mid h)`$ spricht man von

- â€œmultinominalemâ€ NB: Attribute umfassen mehrere Kategorien
  (verschiedene AusprÃ¤gungen, wie im â€œWahlkampfâ€-Beispiel: Attribut
  â€œBildungâ€ hat die AusprÃ¤gungen â€œAbiturâ€, â€œBachelorâ€ und â€œMasterâ€)
- Bernoulli NB: Attribute sind binÃ¤r (AusprÃ¤gung 0 oder 1),
  typischerweise bei der Textklassifikation
- Gaussâ€™sches NB: Annahme einer Normalverteilung der
  Attribut-AusprÃ¤gungen

## Wrap-Up

- Klassifikation mit Naive Bayes
  - Annahme von UnabhÃ¤ngigkeit =\> â€œNaiveâ€ Bayes Klassifikation
  - SchÃ¤tzen der bedingten Wahrscheinlichkeiten aus den Trainingsdaten
  - Klassifikation durch Nutzung der geschÃ¤tzten Wahrscheinlichkeiten
  - Hinweis auf NaivitÃ¤t der Annahme, dennoch sehr gute Erfolge in
    Praxis
  - Hinweis auf Probleme mit niedrigen Wahrscheinlichkeiten

## ğŸ“– Zum Nachlesen

Lesen Sie in ([Russell und Norvig 2021](#ref-Russell2021)) bitte den
Abschnitt 12.6 â€œNaive Bayes Modelsâ€. DarÃ¼ber hinaus ist Abschnitt â€œ8.10
Der Naive Bayes Klassifikatorâ€ ([Ertel 2025](#ref-Ertel2025)) sehr
empfehlenswert.

> [!NOTE]
>
> <details>
>
> <summary><strong>âœ… Lernziele</strong></summary>
>
> - k2: Ich kann die UnabhÃ¤ngigkeits-Annahme in der â€˜Naiveâ€™ Bayes
>   Klassifikation erklÃ¤ren
> - k2: Ich kann die Probleme mit niedrigen Wahrscheinlichkeiten
>   erklÃ¤ren
> - k3: Ich kann die bedingten Wahrscheinlichkeiten aus konkreten
>   Trainingsdaten schÃ¤tzen
> - k3: Ich kann die Klassifikation mit Naive Bayes durch Nutzung der
>   geschÃ¤tzten Wahrscheinlichkeiten durchfÃ¼hren
>
> </details>

> [!TIP]
>
> <details>
>
> <summary><strong>ğŸ§© Quizzes</strong></summary>
>
> - [Selbsttest Naive Bayes Klassifikation
>   (ILIAS)](https://www.hsbi.de/elearning/goto.php?target=tst_1106588&client_id=FH-Bielefeld)
>
> </details>

> [!TIP]
>
> <details>
>
> <summary><strong>ğŸ… Challenges</strong></summary>
>
> **Spam-Mails**
>
> Stellen Sie sich vor, Sie haben eine Sammlung von 100 E-Mails (60
> Spam, 40 Nicht-Spam). Sie wissen, dass das Wort â€œGewinnâ€ in 45
> Spam-E-Mails und in 5 Nicht-Spam-E-Mails vorkommt.
>
> 1.  Berechnen Sie die Wahrscheinlichkeit, dass es eine E-Mail Spam
>     ist, wenn das Wort â€œGewinnâ€ darin vorkommt.
>
> 2.  Wie wÃ¼rde die E-Mail mit dem Wort â€œGewinnâ€ durch einen Naive Bayes
>     Klassifikator bewertet?
>
> **Textklassifikation**
>
> Betrachten Sie die folgenden Aussagen:
>
> > - Patient A hat weder Husten noch Fieber und ist gesund.
> > - Patient B hat Husten, aber kein Fieber und ist gesund.
> > - Patient C hat keinen Husten, aber Fieber. Er ist krank.
> > - Patient D hat Husten und kein Fieber und ist krank.
> > - Patient E hat Husten und Fieber. Er ist krank.
>
> Aufgaben:
>
> 1.  Trainieren Sie auf diesem Datensatz einen Klassifikator mit NB.
> 2.  Ist Patient F krank? Er hat Husten, aber kein Fieber.
>
> </details>

------------------------------------------------------------------------

> [!NOTE]
>
> <details>
>
> <summary><strong>ğŸ‘€ Quellen</strong></summary>
>
> <div id="refs" class="references csl-bib-body hanging-indent">
>
> <div id="ref-Ertel2025" class="csl-entry">
>
> Ertel, W. 2025. *Grundkurs KÃ¼nstliche Intelligenz*. 6th edition.
> Springer Vieweg Wiesbaden.
> <https://doi.org/10.1007/978-3-658-44955-1>.
>
> </div>
>
> <div id="ref-Russell2021" class="csl-entry">
>
> Russell, S., und P. Norvig. 2021. *Artificial Intelligence: A Modern
> Approach*. 4th Edition. Pearson. <http://aima.cs.berkeley.edu>.
>
> </div>
>
> </div>
>
> </details>

------------------------------------------------------------------------

<img src="https://licensebuttons.net/l/by-sa/4.0/88x31.png" width="10%">

Unless otherwise noted, this work is licensed under CC BY-SA 4.0.

<blockquote><p><sup><sub><strong>Last modified:</strong> e5c1685 (lecture: reformat NB, 2026-01-12)<br></sub></sup></p></blockquote>
