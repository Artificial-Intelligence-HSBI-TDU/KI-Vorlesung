# NB: Text-Klassifikation mit Naive Bayes

> [!IMPORTANT]
>
> <details open>
>
> <summary><strong>ğŸ¯ TL;DR</strong></summary>
>
> Der NB-Klassifikator wird gern fÃ¼r die Textklassifikation eingesetzt.
> Hier muss man einem Text ein Label zuordnen.
>
> In einer Vorverarbeitung wird zunÃ¤chst eine Menge der relevanten
> WÃ¶rter Ã¼ber alle Trainingstexte gebildet (*Bag-of-Words*). Der
> Bag-of-Words entspricht einem Merkmalsvektor, wobei die Merkmale die
> einzelnen WÃ¶rter sind. Dann kann jeder Text der Trainingsmenge Ã¼ber so
> einen Merkmalsvektor dargestellt werden: Entweder man gibt pro Merkmal
> an, ob es da (1) oder nicht da (0) ist (Variante Bernoulli NB) oder
> man zÃ¤hlt die HÃ¤ufigkeit des Auftretens der Merkmale (Variante
> Multinomial NB). Dann kann man mit dem NB-Klassifikator die bedingten
> Wahrscheinlichkeiten schÃ¤tzen und einen neuen Text klassifizieren.
>
> Bei beiden Varianten hat man das Problem mit Termen, die fÃ¼r eine
> Klasse im Training nicht vorkommen (Zero-Frequency-Problem). Dabei
> werden die geschÃ¤tzten Wahrscheinlichkeiten zu 0, wodurch bei der
> Verwendung in der Klassifikation entsprechend die Scores ebenfalls zu
> 0 berechnen und eine sinnvolle Klassifikation nicht mÃ¶glich ist.
> Diesem Problem kann man durch die Laplace-GlÃ¤ttung begegnen, welche
> wie kÃ¼nstliche Trainingsdaten wirkt.
>
> Wenn im zu klassifizierenden Testdatensatz WÃ¶rter vorkommen, die nicht
> beim Training vorhanden waren, spricht man auch vom
> â€œOut-of-Vocabularyâ€-Problem. Am einfachsten lÃ¤sst sich dieses Problem
> lÃ¶sen, indem man bereits beim Training einen â€œUNKâ€-Term trainiert (UNK
> steht fÃ¼r â€œunknownâ€). Bei der Klassifikation werden OOV-Terme einfach
> auf den UNK-Term abgebildet und die dafÃ¼r berechneten
> Wahrscheinlichkeiten genutzt.
> </details>

> [!TIP]
>
> <details open>
>
> <summary><strong>ğŸ¦ Videos</strong></summary>
>
> - [VL Text-Klassifikation mit Naive
>   Bayes](https://youtu.be/Gkg-SGnl9nY)
>
> </details>

## Textklassifikation mit NB

- Trainingsdaten: Mails, manuell markiert:
  - D1: (â€œDie sieben Zwerge fraÃŸen die sieben Ziegenâ€, OK)
  - D2: (â€œDie sieben Ziegen traten die sieben WÃ¶lfeâ€, SPAM)
  - D3: (â€œDie sieben WÃ¶lfe fraÃŸen die sieben BÃ¶ckeâ€, OK)
  - D4: (â€œDie sieben BÃ¶cke traten die sieben Zwergeâ€, OK)

<!-- -->

- Testdaten (neue Mails):
  - T1: (â€œDie sieben Zwerge fraÃŸen die sieben WÃ¶lfeâ€)

Lernen Sie mit Hilfe der Trainingsmenge einen Naive-Bayes-Klassifikator
und wenden Sie diesen auf die Test-Dokumente an.

## Vorverarbeitung

- Vereinheitlichung: Kleinbuchstaben

- **Tokenisierung**: Aufteilung auf einzelne WÃ¶rter

  Manchmal gar nicht so einfach: Wie viele WÃ¶rter sind â€œarenâ€™tâ€?

- **Stop Words** entfernen: â€œderâ€, â€œdieâ€, â€œdasâ€, â€¦

  Artikel und FÃ¼llwÃ¶rter enthalten oft keine wirkliche Information fÃ¼r
  die Klassifikationsaufgabe und werden deshalb entfernt.

  Dabei am besten auch Zeichensetzung mit entfernen.

- **Lemmatisierung**: â€œfraÃŸenâ€ -\> â€œfressenâ€, â€œWÃ¶lfeâ€ -\> â€œWolfâ€, â€¦

  Die WÃ¶rter werden auf ihre â€œGrundformâ€ gebracht: gebeugte Verben durch
  den Infinitiv ersetzt oder Plurale durch den Singular ersetzt etc.
  Erfolgt hÃ¤ufig mit Hilfe von WÃ¶rterbÃ¼chern.

- **Synonyme, Homonyme, Akronyme** ersetzen

  Synonyme: Verschiedene WÃ¶rter mit selber/Ã¤hnlicher Bedeutung:
  â€œFahrstuhlâ€, â€œLiftâ€

  Homonyme: Wort mit unterschiedlicher Bedeutung: â€œBankâ€
  (Sitzgelegenheit, Geldinstitut)

  Akronyme: AbkÃ¼rzungen: â€œSPOâ€

- **Stemming**

  WÃ¶rter auf ihre â€œGrundformâ€ zurÃ¼ckbringen durch Abschneiden: Aus
  â€œclosedâ€ oder â€œclosingâ€ wÃ¼rde â€œclosâ€ als gemeinsame Form. Eine etwas
  krude Variante der Lemmatisierung.

- **Terme**: Die erhaltenen WÃ¶rter nennen wir â€œTermeâ€.

## Merkmalsvektor: Bag of Words

Nach Vorverarbeitung:

- Trainingsdaten (**Terme**):
  - D1: (sieben, zwerg, fressen, sieben, ziege; OK)
  - D2: (sieben, ziege, treten, sieben, wolf; SPAM)
  - D3: (sieben, wolf, fressen, sieben, bock; OK)
  - D4: (sieben, bock, treten, sieben, zwerg; OK)
- Testdaten (**Terme**):
  - T1: (sieben, zwerg, fressen, sieben, wolf)

**Vokabular**: V = {sieben, zwerg, fressen, ziege, treten, wolf, bock}

**Bag of Words**: Merkmalsvektor mit fester Anordnung des Vokabulars

## Bag of Words fÃ¼r Trainingsdaten

|     | sieben | zwerg | fressen | ziege | treten | wolf | bock | Klasse |
|:----|:-------|:------|:--------|:------|:-------|:-----|:-----|:-------|
| D1  | 2      | 1     | 1       | 1     | 0      | 0    | 0    | OK     |
| D2  | 2      | 0     | 0       | 1     | 1      | 1    | 0    | SPAM   |
| D3  | 2      | 0     | 1       | 0     | 0      | 1    | 1    | OK     |
| D4  | 2      | 1     | 0       | 0     | 1      | 0    | 1    | OK     |

Im *Bag of Words* (BoW) bekommt jeder Term des Vokabulars einen festen
Platz. Die Trainingsdaten und die Testdaten kÃ¶nnen nun mit Hilfe dieses
Vektors dargestellt werden, indem fÃ¼r jeder Term im BoW die Anzahl der
Vorkommen im Trainings- oder Testvektor gezÃ¤hlt wird (â€œ*Multinomial
NB*â€).

Es gibt auch die einfachere Form, die lediglich das Vorkommen vermerkt,
also mit 0 und 1 arbeitet und nicht durchzÃ¤hlt (â€œ*Bernoulli NB*â€, s.u.).

## Naive Bayes Training (Multinomial NB)

Bei Multinomial NB zÃ¤hlen wir die HÃ¤ufigkeiten der Vorkommen der
einzelnen Terme in den einzelnen Dokumenten der jeweiligen Klasse.

Nachfolgend sind die entsprechenden HÃ¤ufigkeiten im Trainingsdatensatz
zusammengefasst dargestellt.

| Klasse | sieben | zwerg | fressen | ziege | treten | wolf | bock | Anzahl WÃ¶rter | Anzahl Dokumente |
|:-------|:-------|:------|:--------|:------|:-------|:-----|:-----|:--------------|:-----------------|
| OK     | 6      | 2     | 2       | 1     | 1      | 1    | 2    | 15            | 3                |
| SPAM   | 2      | 0     | 0       | 1     | 1      | 1    | 0    | 5             | 1                |

FÃ¼r die Terme ist jeweils die Gesamt-Anzahl des Vorkommens des Terms in
den Dokumenten der jeweiligen Klasse angegeben.

Naive Bayes â€œtrainierenâ€

- A-priori-Wahrscheinlichkeit der Klassen:
  $`P(c) = \dfrac{N_c}{N} = \dfrac{\text{Anzahl Dokumente in Klasse c}}{\text{Anzahl Dokumente}}`$

- Likelihood der Daten (Terme):
  $`P(t \mid c) = \dfrac{\mathop{\text{count}}(t,c)}{\sum_{v \in V} \mathop{\text{count}}(v,c)}`$
  mit $`\mathop{\text{count}}(t,c)`$ Anzahl der Vorkommen von Term $`t`$
  in allen Dokumenten der Klasse $`c`$ und $`V`$ die Vereinigung aller
  Terme aller Dokumente (Vokabular)

<!-- -->

- A-priori-Wahrscheinlichkeit:
  - $`P(\text{OK}) = 3/4 = 0.75`$
  - $`P(\text{SPAM}) = 1/4 = 0.25`$
- Likelihood:
  - $`P(\text{sieben} \mid \text{OK})`$
    $`= (2+2+2)/(2+2+2+1+1+1+1+1+1+1+1+1)`$ $`= 6/15 = 0.40`$
  - $`P(\text{sieben} \mid \text{SPAM})`$
    $`= (2)/(2+1+1+1) = 2/5 = 0.40`$
  - $`P(\text{zwerg} \mid \text{OK}) = 2/15 = 0.133`$
  - $`P(\text{zwerg} \mid \text{SPAM}) = 0/5 = 0.00`$
  - $`P(\text{fressen} \mid \text{OK}) = 2/15 = 0.133`$
  - $`P(\text{fressen} \mid \text{SPAM}) = 0/5 = 0.00`$
  - $`P(\text{ziege} \mid \text{OK}) = 1/15 = 0.067`$
  - $`P(\text{ziege} \mid \text{SPAM}) = 1/5 = 0.20`$
  - $`P(\text{treten} \mid \text{OK}) = 1/15 = 0.067`$
  - $`P(\text{treten} \mid \text{SPAM}) = 1/5 = 0.20`$
  - $`P(\text{wolf} \mid \text{OK}) = 1/15 = 0.067`$
  - $`P(\text{wolf} \mid \text{SPAM}) = 1/5 = 0.20`$
  - $`P(\text{bock} \mid \text{OK}) = 2/15 = 0.133`$
  - $`P(\text{bock} \mid \text{SPAM}) = 0/5 = 0.00`$

## Naive Bayes Klassifikation (Multinomial NB)

$`h_{MAP} = \mathop{\text{argmax}}_{h \in H} \: P(h) \prod_{x \in \mathbf{x}} P(x \mid h)`$

Jedes Vorkommen eines Wortes im Testdatensatz ist ein $`x`$!

``` math
h_{MAP} = \mathop{\text{argmax}}_{h \in H} \: P(h) \prod_{w \in \mathbf{V}:\,\mathop{\text{count}}(w)>0} P(w \mid h)^{\mathop{\text{count}}(w)}
```

T1: (sieben, zwerg, fressen, sieben, wolf)

- **h = OK**:
  $`P(\text{OK}) \cdot P(\text{sieben} \mid \text{OK})^2 \cdot P(\text{zwerg} \mid \text{OK})^1 \cdot P(\text{fressen} \mid \text{OK})^1 \cdot P(\text{ziege} \mid \text{OK})^0 \cdot P(\text{treten} \mid \text{OK})^0 \cdot P(\text{wolf} \mid \text{OK})^1 \cdot P(\text{bock} \mid \text{OK})^0`$
  $`= 0.75*0.40*0.40*0.133*0.133*1*1*0.067*1 = 0.00014221956`$

- **h = SPAM**:
  $`P(\text{SPAM}) \cdot P(\text{sieben} \mid \text{SPAM})^2 \cdot P(\text{zwerg} \mid \text{SPAM})^1 \cdot P(\text{fressen} \mid \text{SPAM})^1 \cdot P(\text{ziege} \mid \text{SPAM})^0 \cdot P(\text{treten} \mid \text{SPAM})^0 \cdot P(\text{wolf} \mid \text{SPAM})^1 \cdot P(\text{bock} \mid \text{SPAM})^0`$
  $`= 0.25*0.40*0.40*0.00*0.00*1*1*0.20*1 = 0.00`$

Entscheidung: OK

> [!TIP]
>
> **Hinweis**: $`P(w \mid h)^{\mathop{\text{count}}(w)}`$ muss so
> gelesen werden:
>
> - Wenn ein Term $`w`$ ein- oder mehrfach im Testvektor vorkommt, wird
>   die zugehÃ¶rige Likelihood entsprechend ein- bzw. mehrfach
>   multipliziert.
> - Wenn ein Term $`w`$ nicht im Testvektor vorhanden ist
>   ($`\mathop{\text{count}}(w) = 0`$), verschwindet die Likelihood und
>   es wird automatisch ein Faktor 1 aufmultipliziert. Alternativ lÃ¤sst
>   man alle Terme mit $`\mathop{\text{count}}(w) = 0`$ einfach in der
>   Formel weg.
>
> Nur tatsÃ¤chlich vorhandene Terme im Testvektor
> ($`\mathop{\text{count}}(w) > 0`$) zahlen in das Ergebnis ein.
>
> Wenn die Likelihood eines Terms 0 ist (Zero-Frequency-Problem), dann
> wird der gesamte Ausdruck zu 0. Durch die beschriebene Lesart von
> $`P(w \mid h)^{\mathop{\text{count}}(w)}`$ ergibt sich dann bei
> $`\mathop{\text{count}}(w) = 0`$ auch kein $`0^0`$-Problem, weil die
> Likelihood gar nicht erst ausgewertet wird bzw. automatisch mit 1
> ersetzt wurde. Dies ist eine Art Implementierungsdetail (rein
> mathematisch lÃ¤sst sich das so nicht ablesen)!

**Beobachtungen**:

1.  Viele kleine Faktoren ergeben ein immer kleineres Produkt =\>
    Logarithmus nutzen und Ãœbergang zu Summe statt Produkt!
2.  Im Trainingsdatensatz nicht vorhandene Terme bei einer Klasse fÃ¼hren
    zu einem Faktor 0 (Zero-Frequency-Problem), wodurch der gesamte
    Score 0 wird. =\> Laplace-GlÃ¤ttung einsetzen!
3.  Im Testdatensatz nicht vorhandene Terme des Vokabulars werden
    automatisch ignoriert, da der *count* 0 ist und als Exponent
    verwendet den neutralen Faktor 1 ergibt.

## Naive Bayes mit Laplace-GlÃ¤ttung (Multinomial NB)

``` math
P(t \mid c) = \dfrac{\mathop{\text{count}}(t,c) + \alpha}{\sum_{v \in V} \mathop{\text{count}}(v,c) + \alpha \cdot \lvert V \rvert},
\quad \text{ bei Laplace: } \alpha = 1
```

- A-priori-Wahrscheinlichkeit:
  - $`P(\text{OK}) = 3/4 = 0.75`$
  - $`P(\text{SPAM}) = 1/4 = 0.25`$
- Likelihood:
  - $`P(\text{sieben} \mid \text{OK})`$
    $`= (2+2+2+\mathbf{1})/(2+2+2+1+1+1+1+1+1+1+1+1+\mathbf{7})`$
    $`= 7/22 = 0.318`$ (vorher: 0.40)
  - $`P(\text{sieben} \mid \text{SPAM})`$
    $`= (2+\mathbf{1})/(2+1+1+1+\mathbf{7})`$ $`= 3/12 = 0.25`$ (vorher:
    0.40)
  - $`P(\text{zwerg} \mid \text{OK}) = (2+\mathbf{1})/(15+\mathbf{7}) = 0.136`$
    (vorher: 0.133)
  - $`P(\text{zwerg} \mid \text{SPAM}) = (0+\mathbf{1})/(5+\mathbf{7}) = 0.083`$
    (vorher: **0.00**)
  - $`P(\text{fressen} \mid \text{OK}) = (2+\mathbf{1})/(15+\mathbf{7}) = 0.136`$
    (vorher: 0.133)
  - $`P(\text{fressen} \mid \text{SPAM}) = (0+\mathbf{1})/(5+\mathbf{7}) = 0.083`$
    (vorher: **0.00**)
  - $`P(\text{ziege} \mid \text{OK}) = (1+\mathbf{1})/(15+\mathbf{7}) = 0.091`$
    (vorher: 0.067)
  - $`P(\text{ziege} \mid \text{SPAM}) = (1+\mathbf{1})/(5+\mathbf{7}) = 0.167`$
    (vorher: 0.20)
  - $`P(\text{treten} \mid \text{OK}) = (1+\mathbf{1})/(15+\mathbf{7}) = 0.091`$
    (vorher: 0.067)
  - $`P(\text{treten} \mid \text{SPAM}) = (1+\mathbf{1})/(5+\mathbf{7}) = 0.167`$
    (vorher: 0.20)
  - $`P(\text{wolf} \mid \text{OK}) = (1+\mathbf{1})/(15+\mathbf{7}) = 0.091`$
    (vorher: 0.067)
  - $`P(\text{wolf} \mid \text{SPAM}) = (1+\mathbf{1})/(5+\mathbf{7}) = 0.167`$
    (vorher: 0.20)
  - $`P(\text{bock} \mid \text{OK}) = (2+\mathbf{1})/(15+\mathbf{7}) = 0.136`$
    (vorher: 0.133)
  - $`P(\text{bock} \mid \text{SPAM}) = (0+\mathbf{1})/(5+\mathbf{7}) = 0.083`$
    (vorher: **0.00**)

T1: (sieben, zwerg, fressen, sieben, wolf)

- **h = OK**:
  $`P(\text{OK}) \cdot P(\text{sieben} \mid \text{OK})^2 \cdot P(\text{zwerg} \mid \text{OK})^1 \cdot P(\text{fressen} \mid \text{OK})^1 \cdot P(\text{ziege} \mid \text{OK})^0 \cdot P(\text{treten} \mid \text{OK})^0 \cdot P(\text{wolf} \mid \text{OK})^1 \cdot P(\text{bock} \mid \text{OK})^0`$
  $`= 0.75*0.318*0.318*0.136*0.136*1*1*0.091*1 = 0.0001276540836`$
  (vorher: 0.00014221956)

- **h = SPAM**:
  $`P(\text{SPAM}) \cdot P(\text{sieben} \mid \text{SPAM})^2 \cdot P(\text{zwerg} \mid \text{SPAM})^1 \cdot P(\text{fressen} \mid \text{SPAM})^1 \cdot P(\text{ziege} \mid \text{SPAM})^0 \cdot P(\text{treten} \mid \text{SPAM})^0 \cdot P(\text{wolf} \mid \text{SPAM})^1 \cdot P(\text{bock} \mid \text{SPAM})^0`$
  $`= 0.25*0.25*0.25*0.083*0.083*1*1*0.167*1 = 0.00001797598438`$
  (vorher: **0.00**)

Entscheidung: OK

**Beobachtungen**:

Laplace-GlÃ¤ttung fÃ¼hrt zu leicht verÃ¤nderten SchÃ¤tzungen, die Berechnung
der Scores erscheint aber plausibler (keine Probleme durch
Zero-Frequency-Terme im Training mehr).

## Naive Bayes mit Out-of-Vocabulary-Termen (OOV, Multinomial NB)

Was passiert, wenn bei der Klassifikation eines Testvektors WÃ¶rter
vorkommen, die es beim Training nicht gab?

T1: (sieben, zwerg, fressen, sieben, wolf, **lecker**)

- **h = OK**:
  $`P(\text{OK}) \cdot P(\text{sieben} \mid \text{OK})^2 \cdot P(\text{zwerg} \mid \text{OK})^1 \cdot P(\text{fressen} \mid \text{OK})^1 \cdot P(\text{ziege} \mid \text{OK})^0 \cdot P(\text{treten} \mid \text{OK})^0 \cdot P(\text{wolf} \mid \text{OK})^1 \cdot P(\text{bock} \mid \text{OK})^0`$
  $`\cdot \mathbf{P(\text{lecker} \mid \text{OK})^1}`$

- **h = SPAM**:
  $`P(\text{SPAM}) \cdot P(\text{sieben} \mid \text{SPAM})^2 \cdot P(\text{zwerg} \mid \text{SPAM})^1 \cdot P(\text{fressen} \mid \text{SPAM})^1 \cdot P(\text{ziege} \mid \text{SPAM})^0 \cdot P(\text{treten} \mid \text{SPAM})^0 \cdot P(\text{wolf} \mid \text{SPAM})^1 \cdot P(\text{bock} \mid \text{SPAM})^0`$
  $`\cdot \mathbf{P(\text{lecker} \mid \text{SPAM})^1}`$

$`\mathbf{P(\text{lecker} \mid \text{OK})}`$ und
$`\mathbf{P(\text{lecker} \mid \text{SPAM})}`$ sind **unbekannt**!

**MÃ¶glichkeiten**:

1.  Da im Training nicht vorgekommen: 0 als Wert fÃ¼r
    $`P(\text{lecker} \mid c)`$ annehmen
2.  Term ignorieren
3.  Ad-hoc Laplace einfÃ¼hren
4.  Mit UNK-Term arbeiten im Training/Klassifikation

Wenn wir wÃ¤hrend der Klassifikation mit einem Term konfrontiert werden,
der beim Training nicht aufgetaucht ist, bekommen wir ein Problem.
Potentiell sind die vier oben dargestellten MÃ¶glichkeiten als LÃ¶sung
denkbar. Diese haben aber unterschiedliche Auswirkungen:

- Problem bei (1): SÃ¤mtliche Scores werden zu 0, d.h. wir kÃ¶nnen uns
  nicht entscheiden.
- Problem bei (2): Der Term wurde durch die Vorverarbeitung nicht
  entfernt, d.h. er trÃ¤gt hÃ¶chstwahrscheinliche eine Bedeutung. Ihn
  einfach zu ignorieren wÃ¼rde bedeuten, dass wir diese Information
  wegwerfen und mÃ¶glicherweise eine Fehlklassifikation durchfÃ¼hren.
- Problem bei (3): In sÃ¤mtlichen Likelihoods steckt die GrÃ¶ÃŸe des
  Vokabulars drin - wir mÃ¼ssten also wÃ¤hrend der Klassifikation ad hoc
  alle Likelihoods neu berechnen!
- Variante (4) ist dagegen relativ elegant â€¦

## Naive Bayes mit UNK-Term (OOV, Multinomial NB)

**Training**

Wir fÃ¼hren einen zusÃ¤tzlichen Term **UNK** (â€œunknownâ€) ein und fÃ¼hren
das Training damit durch.

**Achtung**: Das Vokabular wird dadurch um einen Eintrag grÃ¶ÃŸer - alle
Likelihoods mÃ¼ssen entsprechend angepasst werden!

| Klasse | sieben | zwerg | fressen | ziege | treten | wolf | bock | UNK | Anzahl WÃ¶rter | Anzahl Dokumente |
|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|:---|
| OK | 6 | 2 | 2 | 1 | 1 | 1 | 2 | 0 | 15 | 3 |
| SPAM | 2 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 5 | 1 |

- A-priori-Wahrscheinlichkeit:
  - $`P(\text{OK}) = 3/4 = 0.75`$
  - $`P(\text{SPAM}) = 1/4 = 0.25`$
- Likelihood:
  - $`P(\text{sieben} \mid \text{OK})`$
    $`= (2+2+2+1)/(2+2+2+1+1+1+1+1+1+1+1+1+\mathbf{8})`$
    $`= 7/23 = 0.304`$ (vorher: 0.318)
  - $`P(\text{sieben} \mid \text{SPAM})`$
    $`= (2+1)/(2+1+1+1+\mathbf{8})`$ $`= 3/13 = 0.231`$ (vorher: 0.25)
  - â€¦
  - $`P(\text{UNK} \mid \text{OK}) = (0+1)/(15+\mathbf{8}) = 0.043`$
  - $`P(\text{UNK} \mid \text{SPAM}) = (0+1)/(5+\mathbf{8}) = 0.077`$

**Klassifikation**

Unbekannte Terme werden auf **UNK** gemappt. Dabei fÃ¼r die Exponenten
wieder die Anzahl beachten!

T1: (sieben, zwerg, fressen, sieben, wolf, **lecker**)

- **h = OK**:
  $`P(\text{OK}) \cdot P(\text{sieben} \mid \text{OK})^2 \cdot P(\text{zwerg} \mid \text{OK})^1 \cdot P(\text{fressen} \mid \text{OK})^1 \cdot P(\text{ziege} \mid \text{OK})^0 \cdot P(\text{treten} \mid \text{OK})^0 \cdot P(\text{wolf} \mid \text{OK})^1 \cdot P(\text{bock} \mid \text{OK})^0`$
  $`\cdot \mathbf{P(\text{lecker} \mid \text{OK})^1}`$

- **h = SPAM**:
  $`P(\text{SPAM}) \cdot P(\text{sieben} \mid \text{SPAM})^2 \cdot P(\text{zwerg} \mid \text{SPAM})^1 \cdot P(\text{fressen} \mid \text{SPAM})^1 \cdot P(\text{ziege} \mid \text{SPAM})^0 \cdot P(\text{treten} \mid \text{SPAM})^0 \cdot P(\text{wolf} \mid \text{SPAM})^1 \cdot P(\text{bock} \mid \text{SPAM})^0`$
  $`\cdot \mathbf{P(\text{lecker} \mid \text{SPAM})^1}`$

**Mapping**:

- $`P(\text{lecker} \mid \text{OK})`$ =\>
  $`P(\text{UNK} \mid \text{OK}) = 0.043`$
- $`P(\text{lecker} \mid \text{SPAM})`$ =\>
  $`P(\text{UNK} \mid \text{SPAM}) = 0.077`$

Mit dem UNK-Term kann man das Problem des Out-of-Vocabulary elegant
lÃ¶sen. Im Training berechnet man die Likelihood fÃ¼r diesen Term normal
mit. In der Klassifikation werden Terme in den Daten, die nicht im
Vokabular sind, auf diesen UNK-Term gemappt und dessen Likelihood
entsprechend verwendet.

Erweiterung: Der Bag-of-Words-Ansatz fÃ¼hrt sehr schnell zu riesigen
Vektoren, die nur dÃ¼nn besetzt sind. Um diesem Problem ein wenig
entgegen zu wirken, fÃ¼hrt man hÃ¤ufig bereits beim Training eine Schwelle
fÃ¼r die HÃ¤ufigkeit der Terme der Trainingsmenge ein: Terme, die in ihrer
HÃ¤ufigkeit Ã¼ber der Schwelle liegen, werden normal fÃ¼r die SchÃ¤tzung
verwendet. Terme, die in ihrer HÃ¤ufigkeit unter der Schwelle liegen,
werden auf den UNK-Term abgebildet und dort mitgezÃ¤hlt. Hier macht man
sich das [Zipfâ€™sche Gesetz](https://en.wikipedia.org/wiki/Zipf%27s_law)
zunutze: Wenige WÃ¶rter kommen viel hÃ¤ufiger vor als andere, und die
anderen WÃ¶rter sind sehr selten - wobei man sich durchaus darÃ¼ber
streiten kann, ob die seltenen WÃ¶rter eher eine Art Rauschen darstellen
oder vielleicht gerade durch ihre Seltenheit mit zur TrennschÃ¤rfe
beitragen kÃ¶nnen.

## Naive Bayes Training (Bernoulli NB, mit Laplace)

Bei Bernoulli NB prÃ¼fen wir das Vorkommen der einzelnen Terme. Der
Merkmalsraum ist also binÃ¤r - 0 oder 1.

Im Grunde haben wir hier die selben Varianten wie bei Multinomial NB -
ich zeige hier nachfolgend nur die Kombination von Bernoulli NB und
Laplace-GlÃ¤ttung. FÃ¼r â€œohneâ€ Laplace kann man einfach das $`\alpha = 0`$
setzen. Analog zu der Multinomial NB Variante kann man auch bei
Bernoulli NB mit UNK-Termen arbeiten.

|     | sieben | zwerg | fressen | ziege | treten | wolf | bock | Klasse |
|:----|:-------|:------|:--------|:------|:-------|:-----|:-----|:-------|
| D1  | 1      | 1     | 1       | 1     | 0      | 0    | 0    | OK     |
| D2  | 1      | 0     | 0       | 1     | 1      | 1    | 0    | SPAM   |
| D3  | 1      | 0     | 1       | 0     | 0      | 1    | 1    | OK     |
| D4  | 1      | 1     | 0       | 0     | 1      | 0    | 1    | OK     |

Zusammengefasst ergibt sich damit pro Klasse:

| Klasse | sieben | zwerg | fressen | ziege | treten | wolf | bock | Anzahl Dokumente |
|:-------|:-------|:------|:--------|:------|:-------|:-----|:-----|:-----------------|
| OK     | 3      | 2     | 2       | 1     | 1      | 1    | 2    | 3                |
| SPAM   | 1      | 0     | 0       | 1     | 1      | 1    | 0    | 1                |

FÃ¼r die Terme ist jeweils die Gesamt-Anzahl der Dokumente mit dem Term
und der jeweiligen Klasse angegeben.

Naive Bayes â€œtrainierenâ€

- A-priori-Wahrscheinlichkeit der Klassen:
  $`P(c) = \dfrac{N_c}{N} = \dfrac{\text{Anzahl Dokumente in Klasse c}}{\text{Anzahl Dokumente}}`$

- Likelihood der Daten (Terme):

  - $`P(t=1 \mid c) = \dfrac{\text{Anzahl Dokumente in Klasse c mit Term t} + \alpha}{\text{Anzahl Dokumente in Klasse c} + 2 \cdot \alpha}`$
  - $`P(t=0 \mid c) = 1 - P(t=1 \mid c)`$

<!-- -->

- A-priori-Wahrscheinlichkeit:
  - $`P(\text{OK}) = 3/4 = 0.75`$
  - $`P(\text{SPAM}) = 1/4 = 0.25`$
- Likelihood $`\alpha = 1`$:
  - $`P(\text{sieben}=1 \mid \text{OK}) = (3+1)/(3+2) = 4/5 = 0.80`$
  - $`P(\text{sieben}=1 \mid \text{SPAM}) = (1+1)/(1+2) = 2/3 = 0.667`$
  - $`P(\text{zwerg}=1 \mid \text{OK}) = 3/5 = 0.60`$
  - $`P(\text{zwerg}=1 \mid \text{SPAM}) = 1/3 = 0.333`$
  - $`P(\text{fressen}=1 \mid \text{OK}) = 3/5 = 0.60`$
  - $`P(\text{fressen}=1 \mid \text{SPAM}) = 1/3 = 0.333`$
  - $`P(\text{ziege}=1 \mid \text{OK}) = 2/5 = 0.40`$
  - $`P(\text{ziege}=1 \mid \text{SPAM}) = 2/3 = 0.667`$
  - $`P(\text{treten}=1 \mid \text{OK}) = 2/5 = 0.40`$
  - $`P(\text{treten}=1 \mid \text{SPAM}) = 2/3 = 0.667`$
  - $`P(\text{wolf}=1 \mid \text{OK}) = 2/5 = 0.40`$
  - $`P(\text{wolf}=1 \mid \text{SPAM}) = 2/3 = 0.667`$
  - $`P(\text{bock}=1 \mid \text{OK}) = 3/5 = 0.60`$
  - $`P(\text{bock}=1 \mid \text{SPAM}) = 1/3 = 0.333`$

Bei Bernoulli NB wird auch explizit die Wahrscheinlichkeit fÃ¼r die
Abwesenheit eines Terms berechnet, dies ist einfach
$`P(t=0 \mid c) = 1 - P(t=1 \mid c)`$. Der Ãœbersichtlichkeit halber ist
dies oben nicht explizit angegeben/ausgerechnet.

## Naive Bayes Klassifikation (Bernoulli NB)

$`h_{MAP} = \mathop{\text{argmax}}_{h \in H} \: P(h) \prod_{x \in \mathbf{x}} P(x \mid h)`$

FÃ¼r jedes Wort $`w`$ im Vokabular $`V`$ gibt es die Beobachtung
(Bernoulli-Variable) $`lw \in \lbrace 0, 1 \rbrace`$. ($`lw = 1`$ wenn
Wort $`w`$ im Testdokument vorkommt, $`lw=0`$ sonst.)

``` math
h_{MAP} = \mathop{\text{argmax}}_{h \in H} \: P(h) \prod_{w \in \mathbf{V}} P(lw=1 \mid h)^{lw} \cdot (1 - P(lw=1 \mid h))^{1-lw}
```

Wenn ein Wort $`w`$ im Testdatensatz vorkommt, wird es mit
$`P(lw=1 \mid h)`$ berÃ¼cksichtigt. Sonst mit
$`P(lw=0 \mid h) = (1 - P(lw=1 \mid h))`$.

T1: (sieben, zwerg, fressen, sieben, wolf)

- **h = OK**:
  $`P(\text{OK}) \cdot P(\text{sieben}=1 \mid \text{OK}) \cdot P(\text{zwerg}=1 \mid \text{OK}) \cdot P(\text{fressen}=1 \mid \text{OK}) \cdot P(\text{ziege}=0 \mid \text{OK}) \cdot P(\text{treten}=0 \mid \text{OK}) \cdot P(\text{wolf}=1 \mid \text{OK}) \cdot P(\text{bock}=0 \mid \text{OK})`$
  $`= 0.75*0.80*0.60*0.60*(1-0.40)*(1-0.40)*0.40*(1-0.60) = 0.0124416`$

- **h = SPAM**:
  $`P(\text{SPAM}) \cdot P(\text{sieben}=1 \mid \text{SPAM}) \cdot P(\text{zwerg}=1 \mid \text{SPAM}) \cdot P(\text{fressen}=1 \mid \text{SPAM}) \cdot P(\text{ziege}=0 \mid \text{SPAM}) \cdot P(\text{treten}=0 \mid \text{SPAM}) \cdot P(\text{wolf}=1 \mid \text{SPAM}) \cdot P(\text{bock}=0 \mid \text{SPAM})`$
  $`= 0.25*0.667*0.333*0.333*(1-0.667)*(1-0.667)*0.667*(1-0.333) = 0.0009122091926`$

Entscheidung: OK

**Beobachtungen**:

1.  Viele kleine Faktoren ergeben ein immer kleineres Produkt =\>
    Logarithmus nutzen und Ãœbergang zu Summe statt Produkt!
2.  Im Trainingsdatensatz nicht vorhandene Terme bei einer Klasse fÃ¼hren
    zu einem Faktor 0 (Zero-Frequency-Problem), wodurch der gesamte
    Score 0 wird. =\> Laplace-GlÃ¤ttung einsetzen!
3.  Im Testdatensatz nicht vorhandene Terme des Vokabulars werden
    automatisch korrekt mit $`P(lw=0 \mid h)`$ bzw.
    $`(1 - P(lw=1 \mid h))`$ berÃ¼cksichtigt. (Multinomial NB wÃ¼rde diese
    Terme mit dem Faktor 1 ignorieren.)

## Wrap-Up

- **Vorverarbeitung**: Tokenisierung, Vokabular, Bag of Words (BoW)

- **Multinomial NB**:

  - ZÃ¤hle die Vorkommen eines Terms:
    $`P(t \mid c) = \dfrac{\mathop{\text{count}}(t,c) + \alpha}{\sum_{v \in V} \mathop{\text{count}}(v,c) + \alpha \cdot \lvert V \rvert}`$

  - Klassifikation mit
    $`h_{MAP} = \mathop{\text{argmax}}_{h \in H} \: P(h) \prod_{w \in \mathbf{V}} P(w \mid h)^{\mathop{\text{count}}(w)}`$

  - Im Test nicht vorhandene Terme t aus V werden ignoriert (Faktor 1)

- **Bernoulli NB**:

  - PrÃ¼fe das Vorkommen eines Terms:
    $`P(t=1 \mid c) = \dfrac{\text{Anzahl Dokumente in Klasse c mit Term t} + \alpha}{\text{Anzahl Dokumente in Klasse c} + 2 \cdot \alpha}`$

  - Klassifikation mit
    $`h_{MAP} = \mathop{\text{argmax}}_{h \in H} \: P(h) \prod_{w \in \mathbf{V}} P(lw=1 \mid h)^{lw} \cdot (1 - P(lw=1 \mid h))^{1-lw}`$

  - Im Test nicht vorhandene Terme t aus V werden korrekt mit
    $`P(lw=0 \mid h)`$ berÃ¼cksichtigt

<!-- -->

- Problem mit kleinen Faktoren: **Logarithmus**, Ãœbergang zu Summe
- **Zero-Frequency-Problem**: Laplace-GlÃ¤ttung mit $`\alpha=1`$
- Klassifikation von **Out-of-Vocabulary-Termen**: UNK-Term trainieren

## ğŸ“– Zum Nachlesen

Lesen Sie in ([Russell und Norvig 2021](#ref-Russell2021)) bitte den
Abschnitt 12.6 â€œNaive Bayes Modelsâ€. DarÃ¼ber hinaus ist Abschnitt â€œ8.10
Der Naive Bayes Klassifikatorâ€ ([Ertel 2025](#ref-Ertel2025)) sehr
empfehlenswert.

> [!NOTE]
>
> <details>
>
> <summary><strong>âœ… Lernziele</strong></summary>
>
> - k2: Ich kann die verschiedenen Schritte der Vorverarbeitung erklÃ¤ren
> - k2: Ich kann das Bag-of-Words-Modell erklÃ¤ren
> - k3: Ich kann einen Text in das BoW-Modell Ã¼berfÃ¼hren
> - k3: Ich kann die bedingten Wahrscheinlichkeiten aus konkreten
>   Trainingsdaten schÃ¤tzen: Multinomial NB, Bernoulli NB, mit/ohne
>   Laplace-GlÃ¤ttung
> - k3: Ich kann die Klassifikation von Testdaten mit Naive Bayes durch
>   Nutzung der geschÃ¤tzten Wahrscheinlichkeiten durchfÃ¼hren:
>   Multinomial NB, Bernoulli NB
> - k3: Ich kann dem Out-of-Vocabulary-Problem durch das Arbeiten mit
>   UNK-Termen entgegenwirken
>
> </details>

------------------------------------------------------------------------

> [!NOTE]
>
> <details>
>
> <summary><strong>ğŸ‘€ Quellen</strong></summary>
>
> <div id="refs" class="references csl-bib-body hanging-indent">
>
> <div id="ref-Ertel2025" class="csl-entry">
>
> Ertel, W. 2025. *Grundkurs KÃ¼nstliche Intelligenz*. 6th edition.
> Springer Vieweg Wiesbaden.
> <https://doi.org/10.1007/978-3-658-44955-1>.
>
> </div>
>
> <div id="ref-Russell2021" class="csl-entry">
>
> Russell, S., und P. Norvig. 2021. *Artificial Intelligence: A Modern
> Approach*. 4th Edition. Pearson. <http://aima.cs.berkeley.edu>.
>
> </div>
>
> </div>
>
> </details>

------------------------------------------------------------------------

<img src="https://licensebuttons.net/l/by-sa/4.0/88x31.png" width="10%">

Unless otherwise noted, this work is licensed under CC BY-SA 4.0.

<blockquote><p><sup><sub><strong>Last modified:</strong> 25c5e60 (lecture: add github alert as info box (NB3), 2026-01-12)<br></sub></sup></p></blockquote>
